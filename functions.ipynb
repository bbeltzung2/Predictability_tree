{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5db4c-b67d-42cb-9c1a-6629f06fa6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from bayes_opt import BayesianOptimization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c66182-aae7-42fd-9982-5802b609f60a",
   "metadata": {},
   "source": [
    "### Fonctions arbre de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3cbdb-260e-40e6-8440-28f15be93602",
   "metadata": {},
   "source": [
    "#### Fonction de préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eba3319-fe27-4153-b74a-1ecb892c3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, var_x, var_y, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Prépare les données en séparant les variables indépendantes et dépendantes,\n",
    "    effectue un train-test split, et applique SMOTE pour équilibrer les classes.\n",
    "    \"\"\"\n",
    "    X, y = df[var_x], df[var_y]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train, y_train = SMOTE().fit_resample(X_train, y_train)  # Appliquer SMOTE pour équilibrer les classes\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd80fa-38d2-47e5-9a38-4dbf639e230c",
   "metadata": {},
   "source": [
    "#### Fonction de cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cb2db3-8148-49f1-80e9-82bc070a8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de validation croisée pour l'optimisation bayésienne\n",
    "def dtree_cv(max_depth, min_samples_split, min_samples_leaf, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée pour évaluer l'impact des hyperparamètres de l'arbre de décision\n",
    "    \"\"\"\n",
    "    estimator = DecisionTreeClassifier(\n",
    "        max_depth=int(max_depth),\n",
    "        min_samples_split=int(min_samples_split),\n",
    "        min_samples_leaf=int(min_samples_leaf),\n",
    "        random_state=42\n",
    "    )\n",
    "    cval = cross_val_score(estimator, X_train, y_train, scoring='f1', cv=5)\n",
    "    return cval.mean()  # Retourne la moyenne des scores F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b839b-cb29-4503-9453-64f5126feb2d",
   "metadata": {},
   "source": [
    "#### Fonction pour appliquer l'optimisation bayésienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da84c506-79b6-4293-aa64-570f4445cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_bayesian(param_bounds, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optimisation bayésienne des hyperparamètres d'un arbre de décision.\n",
    "    \"\"\"\n",
    "    # Initialiser l'optimiseur bayésien\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda max_depth, min_samples_split, min_samples_leaf: dtree_cv(\n",
    "            max_depth, min_samples_split, min_samples_leaf, X_train, y_train),\n",
    "        pbounds=param_bounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "\n",
    "    # Lancer l'optimisation\n",
    "    optimizer.maximize(n_iter=15, init_points=5)  # Optimisation avec n_iter itérations et init_points points initiaux\n",
    "\n",
    "    # Obtenir les meilleurs paramètres\n",
    "    best_params_bayes = optimizer.max['params']\n",
    "    best_params_bayes['max_depth'] = int(best_params_bayes['max_depth'])\n",
    "    best_params_bayes['min_samples_split'] = int(best_params_bayes['min_samples_split'])\n",
    "    best_params_bayes['min_samples_leaf'] = int(best_params_bayes['min_samples_leaf'])\n",
    "    best_score_bayes = optimizer.max['target']\n",
    "\n",
    "    print(f\"Best Parameters (Bayesian Optimization): {best_params_bayes}\")\n",
    "    print(f\"Best Score (Bayesian Optimization): {best_score_bayes}\")\n",
    "    \n",
    "    return best_params_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30271d-9125-418b-a777-6638fa057fbb",
   "metadata": {},
   "source": [
    "#### Fonction d'entraînement d'arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d314bf25-2652-45fb-9063-7ac3ba242c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner un modèle d'arbre de décision avec les meilleurs hyperparamètres\n",
    "def train_final_model(X_train, y_train, best_params_bayes):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle d'arbre de décision avec les meilleurs hyperparamètres.\n",
    "    \"\"\"\n",
    "    final_model = DecisionTreeClassifier(\n",
    "        max_depth=best_params_bayes['max_depth'],\n",
    "        min_samples_split=best_params_bayes['min_samples_split'],\n",
    "        min_samples_leaf=best_params_bayes['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ccf00-1d96-4617-9bf9-6ae8cb19a135",
   "metadata": {},
   "source": [
    "#### Fonction d'affichage de l'arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff7dfc0-2d90-4b56-8334-8a064afc4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_clf(model, X_train):\n",
    "    \"\"\"\n",
    "    Génère et affiche un graphique de l'arbre de décision.\n",
    "    \"\"\"\n",
    "    dot_data = tree.export_graphviz(model, out_file=None,\n",
    "                                    feature_names=X_train.columns,\n",
    "                                    filled=True, rounded=True,\n",
    "                                    special_characters=True,\n",
    "                                   class_names=True)\n",
    "    return dot_data\n",
    "# graphviz.Source(dot_data) pour visualiser l'arbre de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2f395-172e-4415-8b80-652e74350ca6",
   "metadata": {},
   "source": [
    "#### Fonction pour afficher la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de237f3-af99-4ae0-9500-71a8e12c8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher la matrice de confusion\n",
    "def plot_conf_mat(y_test, y_pred, normalize=None):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion pour évaluer la performance du modèle.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    \n",
    "    # Affichage de la matrice de confusion\n",
    "    fig, ax = plt.subplots()\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    \n",
    "    # Enlever la grille\n",
    "    ax.grid(False)  # Désactive la grille\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92381eec-c220-4adb-ad3f-fc1ac1a9f39a",
   "metadata": {},
   "source": [
    "#### Fonction principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c75a084-2775-4f53-bf87-5342158e4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, var_x, var_y, param_bounds):\n",
    "    \"\"\"\n",
    "    Protocole complet pour l'optimisation bayésienne et l'évaluation du modèle.\n",
    "    Cette version retourne des objets utiles pour une utilisation ultérieure et affiche les meilleurs hyperparamètres.\n",
    "    \"\"\"\n",
    "    # Préparer les données\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df, var_x, var_y)\n",
    "\n",
    "    # Optimiser les hyperparamètres\n",
    "    best_params_bayes = optimize_bayesian(param_bounds, X_train, y_train)\n",
    "\n",
    "    # Entraîner le modèle final\n",
    "    final_model = train_final_model(X_train, y_train, best_params_bayes)\n",
    "\n",
    "    # Faire des prédictions sur le jeu de test\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    # Évaluation : matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Affichage de l'arbre de décision\n",
    "    graph = graph_clf(final_model, X_train)\n",
    "\n",
    "    # Créer le dictionnaire avec tous les résultats\n",
    "    output = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'best_params_bayes': best_params_bayes,\n",
    "    'final_model': final_model,\n",
    "    'y_pred': y_pred,\n",
    "    'graph': graph,\n",
    "    'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    # Retourner le dictionnaire contenant tout\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
